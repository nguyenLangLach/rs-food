{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a9122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 0 — CÀI ĐẶT (chạy nếu cần)\n",
    "# =========================\n",
    "# đang dùng môi trường mới, chạy dòng pip dưới đây 1 lần.\n",
    "#pip install -q sentence-transformers faiss-cpu pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef89d62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THANH DUOC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 0 — IMPORTS & CONFIG\n",
    "# =========================\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from tqdm.auto import tqdm\n",
    "import faiss\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fca1fdc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: e:\\1KhoaLuan\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current working directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3192f4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: Config(csv_path='data/foods.csv', output_csv='data/encr/foods_with_text.csv', embedding_path='data/encr/foods_embeddings.npy', faiss_index_path='data/encr/foods_faiss.index', sbert_model='VoVanPhuc/sup-SimCSE-VietNamese-phobert-base', cross_encoder_model='cross-encoder/ms-marco-MiniLM-L-6-v2', batch_size=64, use_gpu=False, hnsw_m=32, hnsw_ef_construction=200, hnsw_ef_search=50)\n",
      "GPU available: False\n"
     ]
    }
   ],
   "source": [
    "#cell 1\n",
    "# Config dễ chỉnh\n",
    "@dataclass\n",
    "class Config:\n",
    "    csv_path: str = \"data/foods.csv\"                 # đường dẫn file CSV của bạn\n",
    "    output_csv: str = \"data/encr/foods_with_text.csv\"     # file lưu kèm cột text\n",
    "    embedding_path: str = \"data/encr/foods_embeddings.npy\" # file lưu embeddings\n",
    "    faiss_index_path: str = \"data/encr/foods_faiss.index\"  # file lưu index (tuỳ hệ thống)\n",
    "    sbert_model: str = \"VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\"  # model SBERT (đa ngôn ngữ)\n",
    "    cross_encoder_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # optional re-ranker\n",
    "    batch_size: int = 64\n",
    "    use_gpu: bool = torch.cuda.is_available()\n",
    "    hnsw_m: int = 32\n",
    "    hnsw_ef_construction: int = 200\n",
    "    hnsw_ef_search: int = 50\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Config: {cfg}\")\n",
    "print(\"GPU available:\", cfg.use_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f8e6e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 2 — TIỀN XỬ LÝ & TẠO TEXT\n",
    "# =========================\n",
    "def clean_text(s: str) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n",
    "    s = str(s)\n",
    "    s = re.sub(r\"http\\S+\", \"\", s)\n",
    "    s = re.sub(r\"<[^>]+>\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def make_text(row: pd.Series,\n",
    "              title_weight: int = 2,\n",
    "              include_nutrition: bool = False,\n",
    "              include_meta: bool = False) -> str:\n",
    "    parts = []\n",
    "    title = clean_text(row.get(\"dish_name\") or row.get(\"title\") or \"\")\n",
    "    if title:\n",
    "        for _ in range(max(1, title_weight)):\n",
    "            parts.append(f\"title: {title}\")\n",
    "    ing = clean_text(row.get(\"ingredients\", \"\"))\n",
    "    if ing:\n",
    "        parts.append(f\"ingredients: {ing}\")\n",
    "    tags = clean_text(row.get(\"dish_tags\", \"\") or row.get(\"tags\", \"\"))\n",
    "    if tags:\n",
    "        parts.append(f\"tags: {tags}\")\n",
    "    method = clean_text(row.get(\"cooking_method\", \"\"))\n",
    "    if method:\n",
    "        parts.append(f\"method: {method}\")\n",
    "    desc = clean_text(row.get(\"description\", \"\"))\n",
    "    if desc:\n",
    "        parts.append(f\"desc: {desc}\")\n",
    "    if include_meta:\n",
    "        dtype = clean_text(row.get(\"dish_type\", \"\"))\n",
    "        if dtype:\n",
    "            parts.append(f\"type: {dtype}\")\n",
    "        if \"serving_size\" in row and pd.notna(row[\"serving_size\"]):\n",
    "            parts.append(f\"serving: {clean_text(row['serving_size'])}\")\n",
    "        if \"cooking_time\" in row and pd.notna(row[\"cooking_time\"]):\n",
    "            parts.append(f\"time: {clean_text(row['cooking_time'])}\")\n",
    "    if include_nutrition:\n",
    "        nums = []\n",
    "        for col in (\"calories\", \"fat\", \"fiber\", \"sugar\", \"protein\"):\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                nums.append(f\"{col}:{row[col]}\")\n",
    "        if nums:\n",
    "            parts.append(\"nutrition: \" + \", \".join(nums))\n",
    "    return \" | \".join([p for p in parts if p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5cd4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (4000, 16)\n",
      "Columns: ['food_id', 'dish_name', 'description', 'dish_type', 'serving_size', 'cooking_time', 'ingredients', 'cooking_method', 'dish_tags', 'calories', 'fat', 'fiber', 'sugar', 'protein', 'image_link', 'nutrient_content']\n",
      "Saved file with text column to: data/encr/foods_with_text.csv\n",
      "\n",
      "Sample text (first 5):\n",
      "- title: Mực ống hấp củ đậu | title: Mực ống hấp củ đậu | ingredients: Mực ống lớn 3 con: 600g làm sạch không bỏ da, Củ sắn: 150g, Cà rốt: 30g, Hành lá: 3 cây, Gốc ngò: 5 cây, Gừng: 15g thái lát, Ớt sừng: 1 trái, Hành tím băm: 1 muỗng, Ớt xiêm xanh: 10 trái nhỏ, đập dập, Gia vị: tiêu, đường, bột năng,\n",
      "- title: Chả giò ngũ vị | title: Chả giò ngũ vị | ingredients: Da bò bía tươi 20 muỗngiếng, Thịt bò xay 100g, Giò sống 100g, Lòng đỏ trứng vịt muối 2 cái, Trứng gà 2 quả, Củ năng gọt vỏ 50g, Khoai lang 100g, Hành tỏi phi 3 muỗng, Cọng cần cắt nhuyễn 2 muỗng, Hành lá cắt khúc, ớt cắt sợi, Ớt tabasco, t\n",
      "- title: Canh ba màu | title: Canh ba màu | ingredients: Khoai mì : 200g, Đậu phộng : 100g, Đậu hũ non : 1 hộp, Bí đỏ : 200g, Hành boaro : 1 cây, Rau om, ngò gai, Muối, đường, tiêu, dầu ăn, Hạt nêm Aji-ngon Nấm, Bột ngọt AJI-NO-MOTO | tags: canh chay, món chay dễ làm | method: Phi thơm hành boaro băm,\n",
      "- title: Tàu hủ ky chiên xả ớt | title: Tàu hủ ky chiên xả ớt | ingredients: Tàu hũ ky lá: 50g, Chao trắng: 1 hũ, Bột mì: ½ chén, Mè trắng rang: 1 muỗng, Ngò rí, hành boaro, Sả băm, ớt hiểm băm, Cà chua, dưa leo, rau sống, Dầu ăn, đường, Xốt tương đậu nành LISA | tags: tàu hũ chiên, tau hu ky | method\n",
      "- title: Gỏi ngó sen tôm thịt | title: Gỏi ngó sen tôm thịt | ingredients: Thịt ba chỉ 200g, Tôm sú 200g, Ngó sen non 300g, Cà rốt 50g, Cần tàu 5 nhánh, Đậu phộng rang, giã dập 20g, Rau răm, chanh, ớt sừng cắt sợi, Hành phi, tỏi ớt băm, Tiêu, đường, nước mắm, Bột ngọt AJI-NO-MOTO, Giấm gạo lên men Aji\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 3 — LOAD CSV & TẠO CỘT TEXT\n",
    "# =========================\n",
    "if not os.path.exists(cfg.csv_path):\n",
    "    raise FileNotFoundError(f\"Không tìm thấy {cfg.csv_path} — đặt file foods.csv vào đường dẫn này hoặc chỉnh cfg.csv_path\")\n",
    "\n",
    "df = pd.read_csv(cfg.csv_path)\n",
    "print(\"Dataframe shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "if \"text\" not in df.columns:\n",
    "    df[\"text\"] = df.apply(make_text, axis=1)\n",
    "df.to_csv(cfg.output_csv, index=False)\n",
    "print(\"Saved file with text column to:\", cfg.output_csv)\n",
    "\n",
    "print(\"\\nSample text (first 5):\")\n",
    "for t in df[\"text\"].head(5).tolist():\n",
    "    print(\"-\", t[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "983efff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model on cpu : VoVanPhuc/sup-SimCSE-VietNamese-phobert-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name VoVanPhuc/sup-SimCSE-VietNamese-phobert-base. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing embeddings at data/encr/foods_embeddings.npy shape: (4000, 768)\n",
      "Embeddings shape: (4000, 768)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 4 — LOAD SBERT & ENCODE\n",
    "# =========================\n",
    "device = \"cuda\" if cfg.use_gpu else \"cpu\"\n",
    "print(\"Loading SBERT model on\", device, \":\", cfg.sbert_model)\n",
    "sbert = SentenceTransformer(cfg.sbert_model, device=device)\n",
    "\n",
    "embeddings = None\n",
    "if os.path.exists(cfg.embedding_path):\n",
    "    try:\n",
    "        tmp = np.load(cfg.embedding_path)\n",
    "        if tmp.shape[0] == len(df):\n",
    "            embeddings = tmp\n",
    "            print(\"Found existing embeddings at\", cfg.embedding_path, \"shape:\", embeddings.shape)\n",
    "        else:\n",
    "            print(\"Existing embeddings length != n_rows. Will re-encode.\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not load embeddings file (will re-encode). Error:\", e)\n",
    "\n",
    "if embeddings is None:\n",
    "    texts = df[\"text\"].astype(str).tolist()\n",
    "    batch_size = cfg.batch_size\n",
    "    all_embs = []\n",
    "    print(\"Start encoding texts: n=\", len(texts), \", batch_size=\", batch_size)\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = sbert.encode(batch, convert_to_numpy=True, show_progress_bar=False)\n",
    "        all_embs.append(emb)\n",
    "    embeddings = np.vstack(all_embs)\n",
    "    np.save(cfg.embedding_path, embeddings)\n",
    "    print(\"Saved embeddings to\", cfg.embedding_path)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e066f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4000, 768)\n",
      "First vector (10 dims): [ 0.04742062  0.31501174 -0.13158861  0.1844102  -0.10268987  0.55493146\n",
      " -0.18508497 -0.25136575  0.25044173  0.11955011  0.02844164  0.00164126\n",
      "  0.21102202 -0.05933519 -0.18255445 -0.6203158   0.4196148   0.0999219\n",
      " -0.04565507 -0.09049148]\n",
      "Second vector (10 dims): [ 0.05434306  0.5268317  -0.1334398   0.14432225  0.08203457  0.47928613\n",
      " -0.03867362 -0.21033369  0.37205866  0.12104223 -0.4850851  -0.18961293\n",
      "  0.1887839  -0.0619368  -0.18721479 -0.50146925  0.1427987   0.10388633\n",
      "  0.21189083  0.03822605]\n"
     ]
    }
   ],
   "source": [
    "# xem thử dât embedding ròi\n",
    "embs = np.load(\"data/encr/foods_embeddings.npy\")\n",
    "print(\"Shape:\", embs.shape)   # (3000, 384)\n",
    "print(\"First vector (10 dims):\", embs[0][:20])   # in 10 số đầu\n",
    "print(\"Second vector (10 dims):\", embs[1][:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c76b0cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã xuất embeddings ra data/encr/foods_embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "embs = np.load(\"data/encr/foods_embeddings.npy\")\n",
    "df_embs = pd.DataFrame(embs)\n",
    "df_embs.to_csv(\"data/encr/foods_embeddings.csv\", index=False)\n",
    "print(\"Đã xuất embeddings ra data/encr/foods_embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45497671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension d = 768\n",
      "Using HNSW index\n",
      "Index ntotal: 4000\n",
      "Saved FAISS index to data/encr/foods_faiss.index\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 5 — NORMALIZE & BUILD FAISS INDEX\n",
    "# =========================\n",
    "faiss.normalize_L2(embeddings)\n",
    "d = embeddings.shape[1]\n",
    "print(\"Embedding dimension d =\", d)\n",
    "\n",
    "try:\n",
    "    index = faiss.IndexHNSWFlat(d, cfg.hnsw_m)\n",
    "    index.hnsw.efConstruction = cfg.hnsw_ef_construction\n",
    "    index.hnsw.efSearch = cfg.hnsw_ef_search\n",
    "    print(\"Using HNSW index\")\n",
    "except Exception as e:\n",
    "    print(\"HNSW unavailable, fallback to IndexFlatIP. Error:\", e)\n",
    "    index = faiss.IndexFlatIP(d)\n",
    "\n",
    "index.add(embeddings)\n",
    "print(\"Index ntotal:\", index.ntotal)\n",
    "\n",
    "try:\n",
    "    faiss.write_index(index, cfg.faiss_index_path)\n",
    "    print(\"Saved FAISS index to\", cfg.faiss_index_path)\n",
    "except Exception as e:\n",
    "    print(\"Warning: could not save FAISS index:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20f01d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to load Cross-Encoder (optional): cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Cross-Encoder loaded\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 6 — LOAD Cross-Encoder (optional)\n",
    "# =========================\n",
    "cross_encoder = None\n",
    "try:\n",
    "    print(\"Trying to load Cross-Encoder (optional):\", cfg.cross_encoder_model)\n",
    "    cross_encoder = CrossEncoder(cfg.cross_encoder_model, device=device)\n",
    "    print(\"Cross-Encoder loaded\")\n",
    "except Exception as e:\n",
    "    print(\"Cross-Encoder not loaded (this is OK). Error:\", e)\n",
    "    cross_encoder = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b08ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Cell 7 — RECOMMEND FUNCTION\n",
    "# =========================\n",
    "def recommend(query: str,\n",
    "              model: SentenceTransformer,\n",
    "              index: faiss.Index,\n",
    "              df: pd.DataFrame,\n",
    "              top_k: int = 10,\n",
    "              rerank_top_n: int = 50,\n",
    "              cross_encoder: Optional[CrossEncoder] = None) -> pd.DataFrame:\n",
    "    qv = model.encode([query], convert_to_numpy=True)\n",
    "    faiss.normalize_L2(qv)\n",
    "    search_k = min(top_k if cross_encoder is None else max(top_k, rerank_top_n), index.ntotal)\n",
    "    D, I = index.search(qv, search_k)\n",
    "    ids = I[0]\n",
    "    scores = D[0]\n",
    "    results = df.reset_index(drop=True).loc[ids].copy()\n",
    "    results[\"vector_score\"] = scores\n",
    "    if cross_encoder is not None:\n",
    "        pairs = [[query, t] for t in results[\"text\"].astype(str).tolist()]\n",
    "        rerank_scores = cross_encoder.predict(pairs)\n",
    "        results[\"rerank_score\"] = rerank_scores\n",
    "        results = results.sort_values(\"rerank_score\", ascending=False).head(top_k)\n",
    "    else:\n",
    "        results = results.sort_values(\"vector_score\", ascending=False).head(top_k)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0755715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Query: tôi muôn ăn trái cây dĩa đá bào\n",
      "                         dish_name  vector_score  rerank_score\n",
      "              Đà điểu xốt trái cây      0.943030      5.778545\n",
      "            Trái Cây Trộn Sữa Chua      0.974560      5.293700\n",
      "                Đu đủ trộn gà khìa      0.998812      3.741148\n",
      "                      Trà Trái Cây      1.004556      3.067932\n",
      "                         Bánh Xoài      1.016626      2.848240\n",
      "                      Sữa Chua Mít      0.966799      2.736762\n",
      "                    Sữa Chua Đu Đủ      1.005901      1.957336\n",
      "                        Sinh Tố Bơ      0.979329      1.652818\n",
      "                      Bia Úp Ngược      1.017239      1.310284\n",
      "             Dâu Tươi Dầm Sữa Chua      0.943992      1.113546\n",
      "                         Chuối Sấy      1.010029      0.557612\n",
      "              Sinh Tố Táo Sữa Tươi      0.997739      0.350567\n",
      "             Smoothies trà sữa dâu      0.972962      0.342537\n",
      "  Smoothie Thanh Long Đỏ Nhiệt Đới      0.933523      0.113293\n",
      "            Sinh Tố Chuối Sữa Chua      0.939835      0.109720\n",
      "                Tự Làm Nước Ép Lựu      0.997233     -0.486362\n",
      "                   Cóc Dầm Muối Ớt      0.933937     -0.565799\n",
      "                          Chè Xoài      0.932289     -0.708590\n",
      "                  Dừa Xay Sốt Xoài      0.993023     -1.010920\n",
      "Smoothie Dâu Tây Hạt Chia Giảm Cân      0.938889     -1.161603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_queries = [\n",
    "    \"tôi muôn ăn trái cây dĩa đá bào\",\n",
    "]\n",
    "\n",
    "for q in sample_queries:\n",
    "    print(\"\\n=== Query:\", q)\n",
    "    res = recommend(q, sbert, index, df, top_k=20, rerank_top_n=30, cross_encoder=cross_encoder)\n",
    "    display_cols = [\"dish_name\"] if \"dish_name\" in res.columns else [\"text\"]\n",
    "    display_cols += [\"vector_score\"]\n",
    "    if \"rerank_score\" in res.columns:\n",
    "        display_cols += [\"rerank_score\"]\n",
    "    print(res[display_cols].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf80e7f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved demo recommendations to /mnt/data/recommendation_demo.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell 9 — SANITY CHECK / SAVE RESULTS\n",
    "# =========================\n",
    "all_rows = []\n",
    "for q in sample_queries:\n",
    "    r = recommend(q, sbert, index, df, top_k=5, rerank_top_n=30, cross_encoder=cross_encoder)\n",
    "    for rank, row in enumerate(r.to_dict(\"records\"), start=1):\n",
    "        all_rows.append({\n",
    "            \"query\": q,\n",
    "            \"rank\": rank,\n",
    "            \"dish_name\": row.get(\"dish_name\") or row.get(\"title\") or row.get(\"text\")[:80],\n",
    "            \"vector_score\": row.get(\"vector_score\"),\n",
    "            \"rerank_score\": row.get(\"rerank_score\") if \"rerank_score\" in row else None\n",
    "        })\n",
    "pd.DataFrame(all_rows).to_csv(\"data/encr/recommendation_demo.csv\", index=False)\n",
    "print(\"Saved demo recommendations to /mnt/data/recommendation_demo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1e0cae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loc cong tác\n",
    "# 8. LỌC CỘNG TÁC (COLLABORATIVE FILTERING)\n",
    "# =========================\n",
    "def collaborative_candidates(user_row, users_df, foods_df, top_n=30):\n",
    "    \"\"\"\n",
    "    Lọc cộng tác: lấy các món mà những user giống user hiện tại đã thích.\n",
    "    \"\"\"\n",
    "    user_rated = user_row.get('danh_gia_mon', None)\n",
    "    if pd.isna(user_rated) or not user_rated:\n",
    "        return foods_df\n",
    "    liked_dishes = [x.strip().lower() for x in str(user_rated).split(',') if x.strip()]\n",
    "    similar_users = users_df[users_df['danh_gia_mon'].apply(\n",
    "        lambda x: any(dish in str(x).lower() for dish in liked_dishes) if pd.notna(x) else False\n",
    "    )]\n",
    "    candidate_dishes = set()\n",
    "    for dishes in similar_users['danh_gia_mon']:\n",
    "        if pd.notna(dishes):\n",
    "            for dish in str(dishes).split(','):\n",
    "                candidate_dishes.add(dish.strip().lower())\n",
    "    foods_candidates = foods_df[foods_df['dish_name'].str.lower().isin(candidate_dishes)]\n",
    "    return foods_candidates if not foods_candidates.empty else foods_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6a8b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       dish_name  vector_score\n",
      "1896     Mì Cay 7 Cấp Độ Cực Hot      0.666314\n",
      "39              Mì xào giòn chay      0.841268\n",
      "642   Gỏi hoa chuối thịt gà chay      0.817716\n",
      "2813            Mì Quảng Ăn Chay      0.793400\n",
      "1535                      Mì Bay      0.815184\n",
      "1932      Mì Căn Xào Cà Ri Sả Ớt      0.828340\n",
      "2104      Mì Gói Xào Rau Củ Chay      0.818670\n",
      "2094        Mì Ý Xốt Cà Chua Nấm      0.845230\n",
      "2042        Mì Ý Sốt Cà Chua Nấm      0.837598\n",
      "1036            Mì tàu hũ cá hồi      0.843520\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 9. TEST LỌC CỘNG TÁC\n",
    "# =========================\n",
    "# Đọc dữ liệu user\n",
    "users_df = pd.read_csv(\"data/users_vietnamese.csv\")\n",
    "user = users_df.iloc[0]  # chọn user bất kỳ\n",
    "\n",
    "# Lọc cộng tác lấy danh sách món ăn\n",
    "candidates = collaborative_candidates(user, users_df, df, top_n=30)\n",
    "\n",
    "# Recommend trên danh sách này\n",
    "query = user['mon_yeu_thich'] if pd.notna(user['mon_yeu_thich']) else \"món ngon\"\n",
    "res = recommend(query, sbert, index, candidates, top_k=10, rerank_top_n=20, cross_encoder=cross_encoder)\n",
    "print(res[['dish_name', 'vector_score']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
